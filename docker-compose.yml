services:
  # Unified application service (FastAPI backend + React frontend)
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        # Pass HF_TOKEN from environment variable (optional, for Llama 3.2 1B)
        # Set via: export HF_TOKEN=your_token_here
        # Or in .env file in project root
        HF_TOKEN: ${HF_TOKEN:-}
    container_name: ai-fun-token-wheel
    ports:
      - "8000:8080"
    environment:
      - PYTHONUNBUFFERED=1
    volumes:
      # Cache for Hugging Face models
      - huggingface-cache:/root/.cache/huggingface
    restart: unless-stopped
    networks:
      - ai-fun-network

volumes:
  # Persistent volume for downloaded models
  huggingface-cache:

networks:
  ai-fun-network:
    driver: bridge
